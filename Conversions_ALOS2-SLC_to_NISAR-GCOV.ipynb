{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6195696-d4f9-425c-b7a9-79c7aac18903",
   "metadata": {},
   "source": [
    "# <img src=\"https://upload.wikimedia.org/wikipedia/commons/6/60/NISAR_artist_concept.jpg\" width=400 align=\"left\"/>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/9/9b/NISAR_Mission_Logo.png\" width=400 align=\"left\"/><br><br><br><br><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7ba3b7-d7e0-4f8d-be90-f4fea8db69c0",
   "metadata": {},
   "source": [
    "# Convert ALOS-2 Data to GCOV\n",
    "\n",
    "In preparation for NISAR, this notebook does:\n",
    "- ALOS-2 L1.1 (must be StripMap) to NISAR RSLC with https://github.com/isce-framework/isce3/blob/develop/share/nisar/examples/alos2_to_nisar_l1.py\n",
    "- NISAR DEM staging with https://github.com/isce-framework/isce3/blob/develop/python/packages/nisar/workflows/stage_dem.py \n",
    "- NISAR RSLC to GCOV with  https://github.com/isce-framework/isce3/blob/develop/python/packages/nisar/workflows/gcov.py\n",
    "\n",
    "\n",
    "Requirements:\n",
    "- a CPU or GPU instance in the On-Demand System \n",
    "- updated ISCE3 repo and ISCE3 conda environment with v0.22.0\n",
    "\n",
    "\n",
    "for questions/feedback, contact Alex Christensen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c02870-e128-44d2-8833-de8cd80f3a2d",
   "metadata": {},
   "source": [
    "<a id=\"TOC\"></a>\n",
    "### Table of Contents\n",
    "1.  [Step 1: Setup](#SEC_1)\n",
    "2.  [Step 2: Download ALOS-2 files](#SEC_2)\n",
    "3.  [Step 3: ALOS-2 SLC to NISAR RSLC](#SEC_3)\n",
    "4.  [Step 4: Generate DEM](#SEC_4)\n",
    "5.  [Step 5: NISAR RSLC to GCOV](#SEC_5)\n",
    "6.  [Step 6: Figures](#SEC_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072134b8-a00e-4d8f-89ed-87b2df7ea54d",
   "metadata": {},
   "source": [
    "<a id=\"SEC_1\"></a>\n",
    "\n",
    "# Step 1: Setup\n",
    "\n",
    "\n",
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a41cd0-91b7-4afe-88ad-dbce4b3e941a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Import packages\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import fnmatch\n",
    "import zipfile\n",
    "import h5py\n",
    "from string import Template\n",
    "from osgeo import gdal\n",
    "import numpy as np\n",
    "import math\n",
    "import boto3\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "import asf_search as asf\n",
    "from shapely import Polygon\n",
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "from yaml import safe_load, safe_dump\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ced5984-95cb-447e-ab3d-bf5c1ece8d01",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf8b07d-5a36-4b0c-b2ac-030bc90e8004",
   "metadata": {},
   "source": [
    "Function to configure GCOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5529213f-51ae-45a7-9e4e-993c8e9119f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### If you want to change to UTM coordiates, set utm = True and choose the x and y posting. The default is 20m\n",
    "def setup_gcov_runconfig(fid,DEM,EPSG,uly='',ulx='',lry='',lrx=''):\n",
    "    with open('/home/jovyan/isce3/share/nisar/defaults/gcov.yaml') as f:\n",
    "        doc = safe_load(f)\n",
    "    folder = '_'.join(fid.split('_')[:18])\n",
    "    doc['runconfig']['groups']['input_file_group']['input_file_path']=  f\"{RSLC_dir/folder.replace('GCOV','RSLC')/folder.replace('GCOV','RSLC')}.h5\"\n",
    "    doc['runconfig']['groups']['product_path_group']['sas_output_file']=  f\"{GCOV_dir/fid/fid}.h5\"\n",
    "\n",
    "    doc['runconfig']['groups']['dynamic_ancillary_file_group']['dem_file'] = f\"{DEM_dir/DEM}.vrt\"\n",
    "    doc['runconfig']['groups']['product_path_group']['scratch_path'] = f\"{TMP_dir}/\"\n",
    "    doc['runconfig']['groups']['processing']['geocode']['output_epsg'] = EPSG\n",
    "    doc['runconfig']['groups']['processing']['geocode']['output_posting']['A']['x_posting'] = x_posting\n",
    "    doc['runconfig']['groups']['processing']['geocode']['output_posting']['A']['y_posting'] = y_posting\n",
    "    doc['runconfig']['groups']['processing']['geocode']['save_rtc_anf_gamma0_to_sigma0']= True\n",
    "    doc['runconfig']['groups']['processing']['rtc']['input_terrain_radiometry'] = f\"sigma0\" ##f\"beta0\"\n",
    "    \n",
    "    # doc['']['groups']['processing']['geocode']['save_layover_shadowmask'] = True\n",
    "    # doc['runconfig']['groups']['processing']['rtc']['save_incidence_angle'] = True \n",
    "    # doc['runconfig']['groups']['processing']['rtc']['save_local_inc_angle'] = True \n",
    "    if uly != '':\n",
    "        doc['runconfig']['groups']['processing']['geocode']['top_left']['y_abs'] = uly\n",
    "        doc['runconfig']['groups']['processing']['geocode']['top_left']['x_abs'] = ulx\n",
    "        doc['runconfig']['groups']['processing']['geocode']['bottom_right']['y_abs'] = lry\n",
    "        doc['runconfig']['groups']['processing']['geocode']['bottom_right']['x_abs'] = lrx\n",
    "        # doc['runconfig']['groups']['processing']['geocode']['y_snap'] = ''\n",
    "        # doc['runconfig']['groups']['processing']['geocode']['x_snap'] = ''\n",
    "        print(uly,ulx,lry,lrx)\n",
    "    print('yaml saved to %s' %(GCOV_dir/fid/(fid + '.yaml')))\n",
    "    Path(GCOV_dir/fid).mkdir(parents=True, exist_ok=True)\n",
    "    with open(GCOV_dir/fid/(fid + '.yaml'), 'w') as f:\n",
    "        safe_dump(doc, f, default_flow_style=False)\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15590b4e-2359-4f57-9953-6b2c7aef75b5",
   "metadata": {},
   "source": [
    "## Set directories\n",
    "\n",
    "Files will be stored in ***/home/jovyan***. They will be copied to the ***nisar-st-data-ondemand*** S3 bucket at the end of the notebook with the option to delete local copies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b3d3d1-4f46-403c-b680-628cfcb7c682",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "notebook_dir = Path(os.getcwd())\n",
    "# working_dir = Path('/home/jovyan') \n",
    "working_dir = Path('/scratch/alex_eco_test/')\n",
    "## if too many files, use scratch\n",
    "# working_dir = Path('/scratch/<yourfolder>/') \n",
    "\n",
    "HOME_DIR = os.environ['HOME']\n",
    "output = !nvcc --version\n",
    "if output == ['/bin/bash: nvcc: command not found']:\n",
    "    print('running CPU instance')\n",
    "    ISCE3_BUILD_DIR = os.environ.get('ISCE3_BUILD_DIR', f'{HOME_DIR}/isce3/build')\n",
    "    isce3_env = 'isce3_src_cpu'\n",
    "else:\n",
    "    print('running GPU instance')\n",
    "    ISCE3_BUILD_DIR = os.environ.get('ISCE3_BUILD_DIR', f'{HOME_DIR}/isce3/build')\n",
    "    isce3_env = 'isce3'\n",
    "\n",
    "# jeff_path = '/scratch/jpon/scripts/DAAC_Ondemand_Reformat/renameH5Product_v2.py'\n",
    "jeff_path = '/scratch/alex_eco_test/scripts/renameH5Product_v3.py'\n",
    "\n",
    "## make sure isce3 version is correct  \n",
    "!source activate {isce3_env} && python -c \"import isce3; print(isce3.__version__); \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb08cf6e-c45c-41b3-a14d-85eef43ac7ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "Files are moved to the ***nisar-st-data-ondemand*** S3 bucket:\n",
    "- ALOS-1 zip files are saved to ***/ALOS1_zip/*** \n",
    "- All processed files (L0B, RSLC, GCOV, DEMs) are saved to ***/ALOS1_processed/***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45735e93-ba3c-4d39-a946-8d56256ec4d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bucket_name = \"nisar-st-data-ondemand\"\n",
    "processed_bucket = 'ALOS2_processed'\n",
    "zip_bucket = 'ALOS2_zip/'\n",
    "s3 = boto3.client(\"s3\")\n",
    "response = s3.list_objects_v2(\n",
    "            Bucket=bucket_name,\n",
    "            Prefix = zip_bucket)\n",
    "contents = response.get('Contents')\n",
    "\n",
    "existing_ALOS2_zipfiles = [contents[i].get('Key').split('/')[-1] for i in range(len(contents)) if '.zip' in contents[i].get('Key')]\n",
    "existing_ALOS2_zippaths = [contents[i].get('Key') for i in range(len(contents)) if '.zip' in contents[i].get('Key')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3525da-c42f-4fab-9f71-6654b05bd3bf",
   "metadata": {},
   "source": [
    "## Choose your AOI\n",
    "\n",
    "New directories will be made to store files for this AOI.\n",
    "\n",
    "For ALOS2, AOI names must match the \"Request\" name in the ALOS2 ASF Spreadsheet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602162d6-d82a-4ff8-a14b-923cfa434733",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# aoi = 'Wax Lake Delta_Louisiana'\n",
    "# sheet = 'Wetlands (L1.1)'\n",
    "# sheet = 'Hazards (L1.1)'\n",
    "\n",
    "aoi = 'BONA'\n",
    "sheet = 'Forest (L1.1)'\n",
    "\n",
    "aoi_str = aoi.replace(\" \", \"_\")\n",
    "aoi_dir = working_dir/aoi_str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e80d13b-1dc0-4c12-8ec5-46e8c4ef43fd",
   "metadata": {},
   "source": [
    "<a id=\"SEC_2\"></a>\n",
    "# Step 2: Download ALOS-2 Files\n",
    "\n",
    "\n",
    "## Get the ALOS2 URLs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292c6f64-4067-4c62-a2c6-99de0a0926ed",
   "metadata": {},
   "source": [
    "If you install **openpyxl**  to your isce3 environment, you can search the ALOS2 spreadsheed directly. Otherwise, you need to copy the list of URLS manually\n",
    "\n",
    "The cell below will ask you which path and frame you want to process (if there are more than one available for your area of interest). The options will be listed in the prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4119a66e-8539-4f5a-b9e9-8ae0b70a9fef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Open ALOS-2 spreadsheed, this file is static. If you're looking for the latest ALOS-2 acquisitions, you will need to download the latest spreadsheet\n",
    "xlsx = pd.ExcelFile('/scratch/alex_eco_test/ALOS-2_Restricted_Available_at_ASF_20240604.xlsx')\n",
    "df = pd.read_excel(xlsx,sheet)\n",
    "\n",
    "## Filter the ALOS2 data using name, path, frame, etc\n",
    "df_filtered = df[df['Request']==aoi]\n",
    "\n",
    "## Filter if more than one path\n",
    "paths = df_filtered.groupby('Path').size()\n",
    "path = int(input('Which path to process? %s' %(paths.index.values)))\n",
    "df_filtered = df_filtered[df_filtered['Path']==path]\n",
    "\n",
    "## Filter if more than one frame\n",
    "frames = df_filtered.groupby('Frame').size()\n",
    "frame = int(input('Which frame to process? %s' %(frames.index.values)))\n",
    "df_filtered = df_filtered[df_filtered['Frame']==frame]\n",
    "\n",
    "frame_dir = aoi_dir / str(path) / str(frame) \n",
    "\n",
    "## Filter if more than one Beam\n",
    "beams = df_filtered.groupby('Beam').size()\n",
    "if len(np.unique(beams.index.values)) >1:\n",
    "    beam = input('Which Beam to process? %s' %(beams.index.values))\n",
    "    df_filtered = df_filtered[df_filtered['Beam']==beam]\n",
    "    frame_dir = aoi_dir / str(path) / str(frame) / str(beam)\n",
    "    \n",
    "else:\n",
    "    beam = beams.index.values[0]\n",
    "    frame_dir = aoi_dir / str(path) / str(frame) #/ str(beam)\n",
    "    \n",
    "print('Working directory for Path %s, Frame %s, Beam %s is %s' %(path, frame, beam, frame_dir))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23381be-ec47-4d4e-a5ce-6573d382ae5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_of_ALOS_SLCs = list(df_filtered['Zip Link'])\n",
    "list_of_ALOS_SLCs.sort()\n",
    "\n",
    "print('You will download %s ALOS2 files, which will require at least %s GB. If you do not have enough space, change your working_dir to scratch and rerun the notebook' %(len(list_of_ALOS_SLCs),3.1*len(list_of_ALOS_SLCs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142cc559-362c-4534-8499-f7cfd24c8d36",
   "metadata": {},
   "source": [
    "Or manually enter the list of URLs to download. You can only process one frame at a time since the workflow assumes all images are in a stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94318727-f096-40f3-a19a-4e30e20c3335",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Example using Mount St. Helens\n",
    "\n",
    "# list_of_ALOS_SLCs = ['https://cumulus.asf.alaska.edu/L1.1/A4/0000436021_001001_ALOS2402350910-211104.zip',\n",
    "                     # 'https://cumulus.asf.alaska.edu/L1.1/A4/0000437308_001001_ALOS2404420910-211118.zip',\n",
    "                     # 'https://cumulus.asf.alaska.edu/L1.1/A4/0000461338_001001_ALOS2410630910-211230.zip',\n",
    "                     # 'https://cumulus.asf.alaska.edu/L1.1/A4/0000461340_001001_ALOS2408560910-211216.zip',\n",
    "                     # 'https://cumulus.asf.alaska.edu/L1.1/A4/0000461343_001001_ALOS2406490910-211202.zip',\n",
    "                     # 'https://cumulus.asf.alaska.edu/L1.1/A4/0000538468_001001_ALOS2512060910-231116.zip']\n",
    "\n",
    "# frame = list_of_ALOS_SLCs[0].split('-')[0][-4:]\n",
    "# frame_dir = aoi_dir / str(path) / str(frame) #/ str(beam)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a674b7-2dfa-4b26-8036-0fef044927d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Make local directories for each NISAR product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a8a27e-cfbb-436d-ac03-4eccf658b4ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ALOS2_dir = aoi_dir / 'ALOS2'\n",
    "try:\n",
    "    frame_dir\n",
    "except:\n",
    "    frame_dir = aoi_dir\n",
    "RSLC_dir = frame_dir / 'RSLC'\n",
    "DEM_dir = frame_dir / 'DEM'\n",
    "GCOV_dir = frame_dir / 'GCOV'\n",
    "TMP_dir = frame_dir / 'TMP'\n",
    "\n",
    "Path(frame_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(RSLC_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(ALOS2_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(DEM_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(GCOV_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(TMP_dir).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32a3160-3d4b-424d-be88-5ad1a0a28b58",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "## Download the ALOS2 files using wget\n",
    "\n",
    "### This cell will attempt to download and unzip the ALOS-2 SLC files. It will check if the files already exist in S3 or locally and will only download and unzip if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8af914-2a30-4b67-902d-cca76564430a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "ALOS_ids = []\n",
    "\n",
    "for a in range(len(list_of_ALOS_SLCs)):\n",
    "    filename =  list_of_ALOS_SLCs[a].split('/')[-1]\n",
    "    ALOS_ids.append(('.').join(filename.split('.')[:-1]))\n",
    "    print('Requested File: ', filename)\n",
    "    print(os.path.isdir(ALOS2_dir/filename[:-4]))\n",
    "    print(os.path.isfile(ALOS2_dir/filename))\n",
    "    print((filename in existing_ALOS2_zipfiles))\n",
    "    if os.path.isdir(ALOS2_dir/filename.split('/')[-1][:-4])==True & (filename in existing_ALOS2_zipfiles)==True:\n",
    "        print('ALOS1 is stored on S3 and already available locally and unzipped')\n",
    "    elif (os.path.isfile(ALOS2_dir/filename)==False) & (filename in existing_ALOS2_zipfiles):\n",
    "        i = existing_ALOS2_zipfiles.index(filename)\n",
    "        s3_path_new = existing_ALOS2_zippaths[i]\n",
    "        print('\\tALOS2 zip is already available at S3 PATH: ', s3_path_new)\n",
    "        print('\\tMove ALOS2 zip from S3 to local')\n",
    "        s3.download_file(bucket_name, s3_path_new , ALOS2_dir/filename)\n",
    "        \n",
    "    elif (os.path.isfile(ALOS2_dir/filename)==False) & (filename not in existing_ALOS2_zipfiles):\n",
    "        print('\\tALOS2 is not available anywhere')\n",
    "        print('\\tDownloading ALOS2 zip ')\n",
    "        command = f\"wget -P {ALOS2_dir} -q {list_of_ALOS_SLCs[a]}\"\n",
    "        output = subprocess.check_output(command, shell=True)\n",
    "        print('\\tMoving a copy ALOS2 zip to S3 bucket')\n",
    "        print('\\tMoving a copy ALOS1 zip to S3 bucket')\n",
    "        s3.upload_file(Filename= str(ALOS2_dir / filename), Bucket=bucket_name, Key='%s%s/%s' %(zip_bucket,aoi_str,filename))\n",
    "\n",
    "\n",
    "    elif (os.path.isfile(ALOS2_dir/filename)==True) & (filename not in existing_ALOS2_zipfiles):\n",
    "        print('\\tALOS2 is available locally, but not on S3')\n",
    "        print('\\tMoving a copy ALOS2 zip to S3 bucket')\n",
    "        s3.upload_file(Filename= str(ALOS2_dir / filename), Bucket=bucket_name, Key='%s%s/%s' %(zip_bucket,aoi_str,filename))\n",
    "\n",
    "    else: \n",
    "        print('\\tALOS2 zip file exists locally and on S3')\n",
    "        \n",
    "    if os.path.isdir(ALOS2_dir/filename[:-4])==False:\n",
    "        print('\\tunzipping the ALOS2 file locally')\n",
    "        try:\n",
    "            with zipfile.ZipFile(ALOS2_dir/filename, 'r') as zip_ref:\n",
    "                zip_ref.extractall(ALOS2_dir/filename[:-4])\n",
    "        except:\n",
    "            print('\\tALOS2 zip is bad, re-downloading')\n",
    "            os.remove(ALOS2_dir/filename)\n",
    "            command = f\"wget -P {ALOS2_dir} -q {list_of_ALOS_SLCs[a]}\"\n",
    "            output = subprocess.check_output(command, shell=True)\n",
    "            print('\\t\\tMoving a copy ALOS2 zip to S3 bucket')\n",
    "            s3.upload_file(Filename= str(ALOS2_dir / filename), Bucket=bucket_name, Key='%s%s/%s' %(zip_bucket,aoi_str,filename))\n",
    "            with zipfile.ZipFile(ALOS2_dir/filename, 'r') as zip_ref:\n",
    "                zip_ref.extractall(ALOS2_dir/filename[:-4])\n",
    "        print('\\t\\tdeleting the local zip file')\n",
    "        os.remove(ALOS2_dir/filename) ## comment to remove the alos2 zip file from your local workspace\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af65b7df-5d79-42a9-b287-4002d325c12b",
   "metadata": {},
   "source": [
    "## Get a list of unzipped ALOS-2 files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23858301-de8e-430d-9ea1-e035e702f86d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_matching_frame = [os.path.join(dirpath,f)\n",
    "                for dirpath,dirnames, files in os.walk(ALOS2_dir)\n",
    "                for f in fnmatch.filter(dirnames,'*')]\n",
    "ALOS2folders = [i for i in all_matching_frame if any(j in i for j in ALOS_ids)]\n",
    "ALOS2folders.sort()\n",
    "ALOS2folders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483fe034-9202-4f1f-b98a-a4d7be286570",
   "metadata": {},
   "source": [
    "<a id=\"SEC_3\"></a>\n",
    "\n",
    "# Step 3: ALOS-2 SLC to NISAR RSLC\n",
    "\n",
    "## Convert ALOS2 SLC to NISAR RSLC format\n",
    "Uses ***alos2_to_nisar_l1.py***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7547e9d0-23a1-464e-b920-75670ab3a6c6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "for ALOS2folder in ALOS2folders[:]:\n",
    "    ALOS2_id = ALOS2folder.split('/')[-1]\n",
    "    print('')\n",
    "    print(ALOS2_id)\n",
    "    if (os.path.isfile(f\"{RSLC_dir/ALOS2_id/ALOS2_id}_rslc.h5\")==True) | len(glob.glob(str(RSLC_dir /'*'/ ('*RSLC*20%s*.h5' %(ALOS2_id.split('-')[-1])))))>0:\n",
    "        print('NISAR RSLC already converted')\n",
    "    else:\n",
    "        Path(RSLC_dir/ALOS2_id).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        !source activate {isce3_env} && python {ISCE3_BUILD_DIR}/share/nisar/examples/alos2_to_nisar_l1.py -i {ALOS2folder} -o {RSLC_dir/ALOS2_id/ALOS2_id}_rslc.h5\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0789881b-0895-4d7d-b2d2-e27c7cf7cf37",
   "metadata": {},
   "source": [
    "## Rename RSLCs to NISAR naming conventions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346f347b-73de-4fad-bee5-f530a965b6ad",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_RSLCs = [os.path.join(dirpath,f)\n",
    "                for dirpath,dirnames, files in os.walk(RSLC_dir)\n",
    "                for f in fnmatch.filter(files,'*_rslc.h5')]\n",
    "all_RSLCs.sort()\n",
    "for NISAR_RSLC in all_RSLCs[:]:\n",
    "    print('')\n",
    "    print(NISAR_RSLC)\n",
    "    path = os.path.dirname(NISAR_RSLC) + '/'\n",
    "    !source activate {isce3_env} && python {jeff_path} -f {NISAR_RSLC} -o {path}/\n",
    "    !rm -r {NISAR_RSLC}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadef892-adc9-4e8e-9353-8540a8bb9a9d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_RSLCs = [os.path.join(dirpath,f)\n",
    "                for dirpath,dirnames, files in os.walk(RSLC_dir)\n",
    "                for f in fnmatch.filter(files,'*RSLC*.h5')]\n",
    "all_RSLCs.sort()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1065b195-c286-44d8-a9e5-c5886ce53ab6",
   "metadata": {},
   "source": [
    "## Choose reference image from stack of RSLCs \n",
    "The first image in the stack will be the reference image. The DEM and extent of final GCOV files will be based on this reference image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36eb0521-d3da-4e6c-a0ea-b1bdd60ffc85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ALOS2_ref = all_RSLCs[0].split('/')[-1].split('.')[0]\n",
    "print('')\n",
    "print('Reference Image will be :' ,ALOS2_ref)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1bd920-03ba-495b-b4b2-80d3c13b3714",
   "metadata": {},
   "source": [
    "<a id=\"SEC_4\"></a>\n",
    "\n",
    "# Step 4: Generate DEM\n",
    "\n",
    "## Get a DEM for the first reference image\n",
    "\n",
    "Uses ***stage_dem.py***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082b7233-90df-483a-9832-5cd8754e6104",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.isfile(f\"{DEM_dir/ALOS2_ref}.vrt\")==False:\n",
    "    command = f'source activate {isce3_env} && python {ISCE3_BUILD_DIR}/packages/nisar/workflows/stage_dem.py -p {RSLC_dir/ALOS2_ref/ALOS2_ref}.h5 -o {DEM_dir/ALOS2_ref}.vrt'     \n",
    "    subprocess.check_output(command, shell=True)\n",
    "\n",
    "DEM = ALOS2_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae768f9-8dd0-4920-8be9-272f5117f439",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_posting = ''\n",
    "y_posting = ''\n",
    "EPSG = ''\n",
    "### If you want to change to UTM coordiates, set utm = True and choose the x and y posting. The default is 20m\n",
    "utm = True\n",
    "if utm:\n",
    "    src = gdal.Open(f\"{DEM_dir/ALOS2_ref}.vrt\")\n",
    "    ulx, xres, xskew, uly, yskew, yres  = src.GetGeoTransform()\n",
    "    lrx = ulx + (src.RasterXSize * xres)\n",
    "    lry = uly + (src.RasterYSize * yres)\n",
    "\n",
    "\n",
    "    x1,y1,x2,y2 = math.floor(ulx),math.floor(uly),math.floor(lrx),math.floor(lry)\n",
    "    zone = int(np.ceil((ulx + 180)/6))\n",
    "\n",
    "    if y1>=0:\n",
    "        EPSG = 32600+zone\n",
    "    elif y1<0:\n",
    "        EPSG = 32600+zone\n",
    "\n",
    "    x_posting = 20 ## choose posting\n",
    "    y_posting = 20 ## choose posting\n",
    "    \n",
    "    \n",
    "print('EPSG: ', EPSG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4481a71c-33d7-4246-a7d7-27b30aac5163",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"SEC_5\"></a>\n",
    "\n",
    "# Step 5: NISAR RSLC to GCOV\n",
    "\n",
    "## Run the GCOV processor on the reference image\n",
    "\n",
    "This image will be used to determine the bounding box of the remaining images in the stack\n",
    "\n",
    "Uses ***gcov.py***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17981e8e-acce-4827-8362-e7df9ebd49f3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "ALOS2_ref2 = ALOS2_ref.replace('RSLC','GCOV')\n",
    "\n",
    "print(ALOS2_ref2)\n",
    "setup_gcov_runconfig(ALOS2_ref2,DEM,EPSG)\n",
    "!source activate {isce3_env} && python {ISCE3_BUILD_DIR}/packages/nisar/workflows/gcov.py {GCOV_dir/ALOS2_ref2/ALOS2_ref2}.yaml\n",
    "\n",
    "NISAR_GCOV = glob.glob(str(GCOV_dir / ALOS2_ref2 / '*GCOV*.h5'))[0]\n",
    "\n",
    "!source activate {isce3_env} && python {jeff_path} -f {NISAR_GCOV} -o {GCOV_dir/ALOS2_ref2}/\n",
    "os.remove(NISAR_GCOV)\n",
    "NISAR_GCOV = glob.glob(str(GCOV_dir / ALOS2_ref2 / '*GCOV*.h5'))[0]\n",
    "ALOS2_ref_new = NISAR_GCOV.split('/')[-1].split('.')[0]\n",
    "shutil.move(GCOV_dir / ALOS2_ref2, GCOV_dir / ALOS2_ref_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26a7646-f1ca-468c-8c1c-f9266d5c650c",
   "metadata": {},
   "source": [
    "## Get top left coordinates of bottom right coordinates of reference GCOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382aa198-2fe3-4ad7-93f3-443cb0c810ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f = h5py.File(f\"{GCOV_dir/ALOS2_ref_new/ALOS2_ref_new}.h5\" , \"r\") \n",
    "a_group_key = list(f.keys())[0]\n",
    "ds_x = f[a_group_key]['LSAR']['GCOV']['grids']['frequencyA']['xCoordinates'][()]      # returns as a h5py dataset object\n",
    "ds_y = f[a_group_key]['LSAR']['GCOV']['grids']['frequencyA']['yCoordinates'][()]      # returns as a h5py dataset object\n",
    "\n",
    "ulx = x_posting * round(ds_x[0]/x_posting)\n",
    "lrx = x_posting * round(ds_x[-1]/x_posting)\n",
    "uly = y_posting * round(ds_y[0]/y_posting)\n",
    "lry = y_posting * round(ds_y[-1]/y_posting)\n",
    "\n",
    "print('Force top left to be: %s %s' %(ulx,uly))\n",
    "print('Force top right to be: %s %s' %(lrx,lry))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e7bd85-8e2e-4741-a86e-f95f29ec8202",
   "metadata": {},
   "source": [
    "## Process the remaining RSLC files to GCOV using the bounding box set by the reference GCOV\n",
    "\n",
    "### Also generates the metadata cube layers\n",
    "\n",
    "Uses ***gcov.py***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb60850-d567-4f8e-8eaf-8106f6c7c46e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for ALOS2_RSLC in all_RSLCs[:]:\n",
    "    ALOS2_id = ALOS2_RSLC.split('/')[-1].split('.')[0].replace('RSLC','GCOV')\n",
    "    if crop:\n",
    "        ALOS2_id = ALOS2_id + '_crop'\n",
    "    print('\\n\\n\\n')\n",
    "    print(ALOS2_id)\n",
    "\n",
    "    if os.path.isfile(f\"{GCOV_dir/ALOS2_id/ALOS2_id}.h5\")==False:\n",
    "        GCOV_dir/ALOS2_id/ALOS2_id\n",
    "        ## GCOV processor\n",
    "        print('\\n>>>>>>>gcov.py')\n",
    "        setup_gcov_runconfig(ALOS2_id,DEM,EPSG,uly,ulx,lry,lrx)\n",
    "        !source activate {isce3_env} && python {ISCE3_BUILD_DIR}/packages/nisar/workflows/gcov.py {GCOV_dir/ALOS2_id/ALOS2_id}.yaml\n",
    "\n",
    "        NISAR_GCOV = glob.glob(str(GCOV_dir / ALOS2_id / '*GCOV*.h5'))[0]\n",
    "        \n",
    "        !source activate {isce3_env} && python {jeff_path} -f {NISAR_GCOV} -o {GCOV_dir/ALOS2_id}/\n",
    "        os.remove(NISAR_GCOV)\n",
    "        NISAR_GCOV = glob.glob(str(GCOV_dir / ALOS2_id / '*GCOV*.h5'))[0]\n",
    "        ALOS2_id_new = NISAR_GCOV.split('/')[-1].split('.')[0]\n",
    "        shutil.move(GCOV_dir / ALOS2_id, GCOV_dir / ALOS2_id_new)\n",
    "        shutil.move(GCOV_dir / ALOS2_id_new / (ALOS2_id + '.yaml'), GCOV_dir / ALOS2_id_new / (ALOS2_id_new+ '.yaml'))\n",
    "        \n",
    "        ## Get metadata cubes\n",
    "        print('\\n\\n>>>>>>>get_product_geometry.py')\n",
    "        !source activate {isce3_env} && python {ISCE3_BUILD_DIR}/packages/nisar/workflows/get_product_geometry.py --dem {DEM_dir/DEM}.vrt --od {GCOV_dir/ALOS2_id_new} {NISAR_GCOV}\n",
    "    else:\n",
    "        print('GCOV already processed')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ee1c2a-43ce-4b95-9599-2e005ed67f3d",
   "metadata": {},
   "source": [
    "<a id=\"SEC_6\"></a>\n",
    "\n",
    "# Step 6: Figures\n",
    "\n",
    "## Make Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223aaeb3-5a33-47fb-9320-a73d4116beff",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_GCOVs = glob.glob(str(GCOV_dir/'**'/'*GCOV*.h5'),recursive=True)\n",
    "if crop:\n",
    "    all_GCOVs = [item for item in all_GCOVs if 'crop' in item]\n",
    "else:\n",
    "    all_GCOVs = [item for item in all_GCOVs if 'crop' not in item]\n",
    "all_GCOVs.sort()\n",
    "\n",
    "for NISAR_GCOV in all_GCOVs[:]:\n",
    "    ALOS2_id = NISAR_GCOV.split('/')[-1][:-3]\n",
    "    \n",
    "    print('')\n",
    "    print(ALOS2_id)\n",
    "    f = h5py.File(f\"{GCOV_dir/ALOS2_id/ALOS2_id}.h5\", \"r\") \n",
    "    a_group_key = list(f.keys())[0]\n",
    "    ds_x = f[a_group_key]['LSAR']['GCOV']['grids']['frequencyA']['xCoordinates'][()]      # returns as a h5py dataset object\n",
    "    ds_y = f[a_group_key]['LSAR']['GCOV']['grids']['frequencyA']['yCoordinates'][()]      # returns as a h5py dataset object\n",
    "    HH = f[a_group_key]['LSAR']['GCOV']['grids']['frequencyA']['HHHH'][()] \n",
    "    HV = f[a_group_key]['LSAR']['GCOV']['grids']['frequencyA']['HVHV'][()] \n",
    "    print('Width x Height:' , ds_x.shape, ds_y.shape)\n",
    "    # ulx = x_posting * round(ds_x[0]/x_posting)\n",
    "    # lrx = x_posting * round(ds_x[-1]/x_posting)\n",
    "    # uly = y_posting * round(ds_y[0]/y_posting)\n",
    "    # lry = y_posting * round(ds_y[-1]/y_posting)\n",
    "    extent= (ds_x[0],ds_y[0],ds_x[-1],ds_y[-1]) \n",
    "    print('Top Left: ', ds_x[0],ds_y[0])\n",
    "    print('Bottom Right: ', ds_x[-1],ds_y[-1])\n",
    "    \n",
    "    fig, [ax1,ax2] = plt.subplots(1,2,figsize = (15,10))\n",
    "    im1 = ax1.imshow(HH, interpolation = 'nearest',\n",
    "          extent = extent, vmin=0,vmax=0.5,cmap='gray')\n",
    "    cbar1 = plt.colorbar(im1, ax=ax1,\n",
    "            shrink=0.5, orientation='horizontal',\n",
    "            pad=0.1, aspect=50,label='HH')\n",
    "    im2 = ax2.imshow(HV, interpolation = 'nearest',\n",
    "          extent = extent, vmin=0,vmax=0.1,cmap='gray')\n",
    "    cbar2 = plt.colorbar(im2, ax=ax2,\n",
    "            shrink=0.5, orientation='horizontal',\n",
    "            pad=0.1, aspect=50,label='HV')\n",
    "    plt.suptitle(ALOS2_id)\n",
    "    if crop:\n",
    "        plt.savefig(GCOV_dir/ALOS2_id/ (ALOS2_id + '_HH-HV_crop.png'))\n",
    "        # !aws s3 cp {GCOV_dir/ ALOS2_id/(ALOS2_id + '_HH-HV_crop.png')} s3://nisar-st-data-ondemand/{processed_bucket}/{'/'.join(str((GCOV_dir/ALOS2_id/ (ALOS2_id + '_HH-HV_crop.png'))).split('/')[3:])}\n",
    "\n",
    "    else:\n",
    "        plt.savefig(GCOV_dir/ALOS2_id/ (ALOS2_id + '_HH-HV.png'))\n",
    "        # !aws s3 cp {GCOV_dir/ ALOS2_id/(ALOS2_id + '_HH-HV.png')} s3://nisar-st-data-ondemand/{processed_bucket}/{'/'.join(str((GCOV_dir/ALOS2_id/ (ALOS2_id + '_HH-HV.png'))).split('/')[3:])}\n",
    "\n",
    "\n",
    "    ## Move figure to S3 bucket\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1d872c-5054-4665-ad04-edc9e5ef56fa",
   "metadata": {},
   "source": [
    "## Move all files to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537aead4-673e-43e2-8e71-caefe54afd51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## move entire folder to S3\n",
    "all_files = [os.path.join(dirpath,f)\n",
    "            for dirpath,dirnames, files in os.walk(aoi_dir)\n",
    "            for f in fnmatch.filter(files,'*')]\n",
    "\n",
    "for file in all_files:\n",
    "    if ('ALOS2' not in file) & ('TMP' not in file):\n",
    "        # print(f\"s3://nisar-st-data-ondemand/{processed_bucket}/{'/'.join((file).split('/')[3:])}\")\n",
    "        !aws s3 cp {file} s3://nisar-st-data-ondemand/{processed_bucket}/{'/'.join((file).split('/')[3:])}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bb0664-980c-420e-a4d6-b62e0082a999",
   "metadata": {},
   "source": [
    "## Clean TMP files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c108fbb-f050-4a35-9db5-d66e86cb8461",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# shutil.rmtree(TMP_dir) \n",
    "# shutil.rmtree(ALOS2_dir)\n",
    "shutil.rmtree(aoi_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d378fbab-61c0-49d1-89d4-79a68f5f33cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644e41b0-1a8a-4348-b9e8-a871184bdbfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7bb6c1-5da6-47c7-a108-8aa36689015b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isce3_src_cpu",
   "language": "python",
   "name": "isce3_src_cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
