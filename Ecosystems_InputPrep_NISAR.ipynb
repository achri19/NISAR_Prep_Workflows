{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"TOP\"></a>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/6/60/NISAR_artist_concept.jpg\" width=150/><img src=\"https://upload.wikimedia.org/wikipedia/commons/9/9b/NISAR_Mission_Logo.png\" width=200/> \n",
    "\n",
    "***\n",
    "\n",
    "# NASA ISRO Synthetic Aperture Radar Mission\n",
    "## Query and Download from ASF DAAC (disabled until NISAR data on ASF)\n",
    "## Preprocess NISAR-simulated ALOS2 products \n",
    "#### This notebook uses a stack of GCOV products that were created with the notebook: Ecosystems_InputPrep_ALOS2-NISAR.ipynb. The files were uploaded to the scratch space. \n",
    "\n",
    "Authors: Alex Christensen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 &emsp; Import Python Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import fnmatch\n",
    "import sys\n",
    "\n",
    "# notebook_dir = Path(os.getcwd())\n",
    "notebook_dir = Path('/scratch/alex_eco_test/')\n",
    "main_dir = notebook_dir.resolve().parents[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from osgeo import gdal, osr, ogr\n",
    "import subprocess\n",
    "\n",
    "import time\n",
    "import scipy\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from IPython.display import Image\n",
    "# import sklearn  # imported from scikit-learn\n",
    "# from sklearn import metrics\n",
    "from matplotlib.pyplot import figure\n",
    "from matplotlib.colors import ListedColormap\n",
    "%matplotlib inline\n",
    "import rasterio\n",
    "\n",
    "# from ipywidgets import interactive\n",
    "from rasterio.plot import show_hist\n",
    "\n",
    "# import asf_search as asf\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import shape, Polygon\n",
    "\n",
    "# from ipyleaflet import (\n",
    "#     Map,  basemaps,\n",
    "#     Rectangle,\n",
    "#     GeoJSON,\n",
    "#     DrawControl,GeoData\n",
    "# )\n",
    "# import ipyleaflet\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2:  Set the Query Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1: Define the AOI\n",
    "Provide either a geojson defining the AOI or draw one on the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# m = Map(basemap=basemaps.Esri.WorldImagery, center=(0,0), zoom=2)\n",
    "# # poly_color = '#00F'\n",
    "# draw_control = DrawControl()\n",
    "\n",
    "# draw_control.rectangle = {\"shapeOptions\": {\"fillColor\": \"#fca45d\", \"color\": \"#fca45d\", \"fillOpacity\": 1.0 }}\n",
    "\n",
    "# def clear_m():\n",
    "#     global rects,polys\n",
    "#     rects = set()\n",
    "#     polys = set()\n",
    "\n",
    "# clear_m()\n",
    "# def handle_draw(self, action, geo_json):\n",
    "#     global rects,polys\n",
    "#     polygon=[]\n",
    "#     for coords in geo_json['geometry']['coordinates'][0][:-1][:]:\n",
    "#         polygon.append(tuple(coords))\n",
    "#     polygon = tuple(polygon)\n",
    "#     if geo_json['properties']['style']['color'] == '#00F':  # poly\n",
    "#         if action == 'created':\n",
    "#             polys.add(polygon)\n",
    "#         elif action == 'edited':\n",
    "#             #polys.update(polygon)\n",
    "#             polys.clear()\n",
    "#             polys.add(polygon)\n",
    "#         elif action == 'deleted':\n",
    "#             polys.discard(polygon)\n",
    "\n",
    "# draw_control.on_draw(handle_draw)\n",
    "# m.add_control(draw_control)\n",
    "  \n",
    "# m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if os.path.isfile('%s.geojson' %(aoi_inputs/aoi))==False:\n",
    "#     polygon: Polygon = shape(draw_control.last_draw.get('geometry'))\n",
    "#     polygon.geom_type\n",
    "#     print(polygon)\n",
    "#     poly_df = gpd.GeoDataFrame(geometry=[polygon],crs='epsg:4326')\n",
    "#     poly_df.to_file('%s.geojson' %(aoi_inputs/aoi),driver='GeoJSON')\n",
    "# else:\n",
    "#     gdf = gpd.read_file('%s.geojson' %(aoi_inputs/aoi))\n",
    "#     polygon = gdf.geometry[0]\n",
    "\n",
    "# wkt = str(polygon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2: Define the search query for NISAR data\n",
    "Options (more options and details are available here: https://docs.asf.alaska.edu/api/keywords/)\n",
    "- platform\n",
    "- instrument\n",
    "- frame\n",
    "- processinglevel\n",
    "- flightDirection\n",
    "- start\n",
    "- end\n",
    "- maxResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# opts = {\n",
    "#     'platform':asf.PLATFORM.SENTINEL1,\n",
    "#     'processingLevel': asf.PRODUCT_TYPE.GRD_HD,\n",
    "#     'flightDirection':asf.FLIGHT_DIRECTION.ASCENDING,\n",
    "#     'start':'Feb 12, 2017',\n",
    "#     'end':'December 11, 2017',\n",
    "#     # 'maxResults':5\n",
    "#     }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3: Submit search and decide whether or not to download\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# results = asf.geo_search(intersectsWith=wkt, **opts)\n",
    "# print(f'{len(results)} results found')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4: Download files.\n",
    "You must enter your ASF/EarthData login credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ## Change to True if you want to download all of the results. Otherwise, adjust search query before continuing\n",
    "# download = False\n",
    "# if download:\n",
    "#     session = asf.ASFSession().auth_with_creds('achri', '**')\n",
    "#     results.download(path=aoi_dir, session=session)\n",
    "# else:\n",
    "#     print('Not downloading yet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Preprocess GCOV products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1: Find, unzip, and open HDF5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aoi = 'Panama Mangrove'\n",
    "sheet = 'Wetlands (L1.1)'\n",
    "\n",
    "aoi_str = aoi.replace(\" \", \"_\")\n",
    "\n",
    "aoi_dir = notebook_dir/aoi_str\n",
    "Path(aoi).mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "print(aoi_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install openpyxl\n",
    "import pandas as pd\n",
    "xlsx = pd.ExcelFile('/scratch/alex_eco_test/ALOS-2_Restricted_Available_at_ASF_20231227.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_filtered = df[(df['Frame']==580) & (df['Path']==51)]\n",
    "df = pd.read_excel(xlsx,sheet)\n",
    "df_filtered = df[df['Request']==aoi]\n",
    "paths = df_filtered.groupby('Path').size()\n",
    "path = int(input('Which path to process? %s' %(paths.index.values)))\n",
    "df_filtered = df_filtered[df_filtered['Path']==path]\n",
    "\n",
    "frames = df_filtered.groupby('Frame').size()\n",
    "frame = int(input('Which frame to process? %s' %(frames.index.values)))\n",
    "df_filtered = df_filtered[df_filtered['Frame']==frame]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aoi_dir = aoi_dir / str(path) / str(frame) \n",
    "\n",
    "input_dir = aoi_dir / 'GCOV'\n",
    "output_dir = aoi_dir / 'GCOV_stacks'\n",
    "# ancillary_dir = main_dir / 'ancillary_data'\n",
    "Path(input_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "print(input_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "h5_files = glob.glob(str(input_dir/ '*gcov_*.h5'))\n",
    "# h5_files = s3.glob(str(aoi_inputs/ 'GCOV*.h5'))\n",
    "print(*h5_files,sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## clean up previously processed geotiffs\n",
    "clean = True\n",
    "if clean:\n",
    "    old_files = glob.glob('%s/*.tif' %(output_dir))\n",
    "    for i in range(len(old_files)):\n",
    "        os.system('rm -r %s' %(old_files[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f = h5py.File(h5_files[0], \"r\") \n",
    "a_group_key = list(f.keys())[0]\n",
    "f[a_group_key]['LSAR']['GCOV']['grids']['frequencyA'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(h5_files)):\n",
    "    filename = h5_files[i].split('/')[-1].split('_')\n",
    "    startdate = filename[-1][:-4]\n",
    "    # enddate = filename[-6]\n",
    "    \n",
    "    # f = h5py.File(s3.open(h5_files[i], \"rb\"))\n",
    "    f = h5py.File(h5_files[i], \"r\") \n",
    "    a_group_key = list(f.keys())[0]\n",
    "    ds_x = f[a_group_key]['LSAR']['GCOV']['grids']['frequencyA']['xCoordinates'][()]      # returns as a h5py dataset object\n",
    "    ds_y = f[a_group_key]['LSAR']['GCOV']['grids']['frequencyA']['yCoordinates'][()]      # returns as a h5py dataset object\n",
    "    # ds_epsg = f[a_group_key]['LSAR']['GCOV']['metadata']['radarGrid']['epsg']\n",
    "    ds_epsg = f[a_group_key]['LSAR']['GCOV']['grids']['frequencyA']['projection'][()]\n",
    "    ds_HHHH = f[a_group_key]['LSAR']['GCOV']['grids']['frequencyA']['HHHH'][()]  # returns as a numpy array\n",
    "    ds_HVHV = f[a_group_key]['LSAR']['GCOV']['grids']['frequencyA']['HVHV'][()]  # returns as a numpy array\n",
    "    \n",
    "    print(h5_files[i].split('/')[-1])\n",
    "    # print('Dates: ', startdate, ' - ',enddate)\n",
    "    print('Raster bounds: ',min(ds_x),max(ds_x),min(ds_y),max(ds_y))\n",
    "    print('X Size: ',ds_x.shape[0],' Y Size: ',ds_y.shape[0])\n",
    "    print('Resolution: ', ds_y[0] - ds_y[1],'m')\n",
    "    print('')\n",
    "    meta = {'driver': 'GTiff', \n",
    "            'dtype': 'float32', \n",
    "            'nodata': None, \n",
    "            'width': ds_x.shape[0], \n",
    "            'height': ds_y.shape[0], \n",
    "            'count': 1, \n",
    "            'crs': rasterio.CRS.from_epsg(ds_epsg[()]), \n",
    "            'transform': rasterio.Affine(ds_x[1] - ds_x[0], 0.0, ds_x[0], 0.0, ds_y[1] - ds_y[0], ds_y[0]), \n",
    "            'tiled': False, \n",
    "            'interleave': 'band'}\n",
    "    fig,[ax1,ax2] = plt.subplots(ncols=2)\n",
    "    ax1.imshow(ds_HHHH,vmin=0,vmax=0.2)\n",
    "    ax1.set_title('HHHH')\n",
    "    ax2.imshow(ds_HVHV,vmin=0,vmax=0.1)\n",
    "    ax2.set_title('HVHV')\n",
    "    plt.show()\n",
    "    with rasterio.open('%s_HHHH.tif' %(output_dir/h5_files[i].split('/')[-1][:-3]), 'w', **meta) as dst:\n",
    "            dst.write(ds_HHHH,indexes=1)    \n",
    "    with rasterio.open('%s_HVHV.tif' %(output_dir/h5_files[i].split('/')[-1][:-3]), 'w', **meta) as dst:\n",
    "        dst.write(ds_HVHV,indexes=1)    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2: Crop reference image and resample/coregister all images in stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "crop = False\n",
    "if crop:\n",
    "    crop_to = gpd.read_file(glob.glob(str(aoi_dir/ '*.geojson'))[0])\n",
    "    crop_to = crop_to.to_crs(rasterio.CRS.from_epsg(ds_epsg[()]))\n",
    "\n",
    "    crop_to.plot()\n",
    "    crop_to = crop_to.explode()\n",
    "    shapes = crop_to.geometry\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3: Get Target Resolution and Target Extent from the Reference (first image in the stack)\n",
    "\n",
    "If you want to change the resolution, set ref_tr manually\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tif_files = glob.glob('%s/*.tif' %(output_dir))\n",
    "tif_files = [item for item in tif_files if 'subset' not in item]\n",
    "if crop:\n",
    "    with rasterio.open(tif_files[0]) as src:\n",
    "        out_image, out_transform = rasterio.mask.mask(src, shapes, crop=crop)\n",
    "        ref_meta = src.meta\n",
    "\n",
    "    ref_meta.update({\"driver\": \"GTiff\",\n",
    "                \"height\": out_image.shape[1],\n",
    "                \"width\": out_image.shape[2],\n",
    "                \"transform\": out_transform})\n",
    "else:\n",
    "    with rasterio.open(tif_files[0]) as src:\n",
    "        out_image = src.read()\n",
    "        out_transform = src.transform\n",
    "        ref_meta = src.meta\n",
    "        \n",
    "with rasterio.open('%s_subset.tif' %(tif_files[0][:-4]), \"w\", **ref_meta) as dest:\n",
    "    dest.write(out_image)\n",
    "\n",
    "with rasterio.open('%s_subset.tif' %(tif_files[0][:-4])) as src:\n",
    "    ref_te = src.bounds\n",
    "    ref_tr = out_transform[0]\n",
    "\n",
    "# ref_tr = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(tif_files)):\n",
    "    os.system('gdalwarp -overwrite -tr %s %s -te %s %s %s %s -tap -srcnodata nan -dstnodata nan %s %s_subset_coreg.tif' %(ref_tr,ref_tr,ref_te.left,ref_te.bottom,ref_te.right,ref_te.top,tif_files[i],tif_files[i][:-4]))\n",
    "    # os.system('gdalwarp -overwrite -tr %s %s -te %s %s %s %s -tap -srcnodata nan -dstnodata nan %s_HVHV.tif %s_HVHV_subset_coreg.tif'  %(ref_tr,ref_tr,ref_te.left,ref_te.bottom,ref_te.right,ref_te.top,h5_files[i][:-4],h5_files[i][:-4]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4: Clean up extra files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(tif_files)):\n",
    "    if i==0:\n",
    "        os.system('rm -r %s_subset.tif' %(tif_files[i][:-4]))\n",
    "    os.system('rm -r %s.tif' %(tif_files[i][:-4]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5: Plot an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HH_files = glob.glob('%s/*HHHH_*_coreg.tif' %(output_dir))\n",
    "HV_files = glob.glob('%s/*HVHV_*_coreg.tif' %(output_dir))\n",
    "HH_files.sort()\n",
    "HV_files.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(HH_files)):\n",
    "    HH = rasterio.open(HH_files[0]).read(1)\n",
    "    HV = rasterio.open(HV_files[0]).read(1)\n",
    "\n",
    "    fig, [ax1,ax2,ax3] = plt.subplots(1,3,figsize=(20,60))\n",
    "\n",
    "    hmin = np.nanmin(HH)\n",
    "    hmax = np.nanquantile(HH,0.75)\n",
    "    vmin = np.nanmin(HV)\n",
    "    vmax = np.nanquantile(HV,0.75)\n",
    "    im1 = ax1.imshow(HH,vmin=hmin,vmax=hmax)\n",
    "    im2 = ax2.imshow(HV,vmin=vmin,vmax=vmax)\n",
    "    im3 = ax3.imshow(HH/HV,vmin=0,vmax=50)\n",
    "    ax1.set_title('HHHH')\n",
    "    ax2.set_title('HVHV')\n",
    "    ax3.set_title('HH/HV')\n",
    "\n",
    "    divider = make_axes_locatable(ax1)\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "    fig.colorbar(im1, cax=cax, orientation='vertical')\n",
    "\n",
    "    divider = make_axes_locatable(ax2)\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "    fig.colorbar(im2, cax=cax, orientation='vertical');\n",
    "    \n",
    "    divider = make_axes_locatable(ax3)\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "    fig.colorbar(im3, cax=cax, orientation='vertical')\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move files to S3 Bucket\n",
    "\n",
    "Uncomment to move files from scratch to S3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [os.path.join(dirpath,f)\n",
    "            for dirpath,dirnames, files in os.walk(aoi_dir)\n",
    "            for f in fnmatch.filter(files,'*')]\n",
    "for file in files:\n",
    "    new_file = ('/').join(file.split('/')[-5:])\n",
    "    command = 'aws s3 mv %s s3://nisar-st-data-ondemand/ALOS2_processed/%s' %(file,new_file)\n",
    "    # print(command)\n",
    "    os.system(command)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecosystems_atbd",
   "language": "python",
   "name": "ecosystems_atbd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
